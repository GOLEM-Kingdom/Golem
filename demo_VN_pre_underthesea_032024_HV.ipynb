{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7g9IV1reyxiw",
        "outputId": "09d58b39-f0b3-4850-f445-8544e6dc3acd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/gdrive/MyDrive/LDS0/Topic_2_3_2_1/demo'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUVJNzpvj7-n",
        "outputId": "a4ce09f8-ee06-41c9-bab2-82b6877bf477"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/LDS0/Topic_2_3_2_1/demo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tiá»n xá»­ lÃ½ dá»¯ liá»‡u tiáº¿ng Viá»‡t"
      ],
      "metadata": {
        "id": "VxorV5ADJfIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install underthesea\n",
        "!pip install demoji\n",
        "!pip install pyvi"
      ],
      "metadata": {
        "id": "4ZPNbAQry57x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eG1CUO_Yyr6t"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from underthesea import word_tokenize, pos_tag, sent_tokenize\n",
        "import regex\n",
        "import demoji\n",
        "from pyvi import ViPosTagger, ViTokenizer\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CofhEKVtyr6x"
      },
      "outputs": [],
      "source": [
        "# https://underthesea.readthedocs.io/en/v1.1.5/readme.html\n",
        "# https://github.com/undertheseanlp/underthesea"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kh8pRQOXyr6y"
      },
      "outputs": [],
      "source": [
        "##LOAD EMOJICON\n",
        "file = open('files/emojicon.txt', 'r', encoding=\"utf8\")\n",
        "emoji_lst = file.read().split('\\n')\n",
        "emoji_dict = {}\n",
        "for line in emoji_lst:\n",
        "    key, value = line.split('\\t')\n",
        "    emoji_dict[key] = str(value)\n",
        "file.close()\n",
        "#################\n",
        "#LOAD TEENCODE\n",
        "file = open('files/teencode.txt', 'r', encoding=\"utf8\")\n",
        "teen_lst = file.read().split('\\n')\n",
        "teen_dict = {}\n",
        "for line in teen_lst:\n",
        "    key, value = line.split('\\t')\n",
        "    teen_dict[key] = str(value)\n",
        "file.close()\n",
        "###############\n",
        "#LOAD TRANSLATE ENGLISH -> VNMESE\n",
        "file = open('files/english-vnmese.txt', 'r', encoding=\"utf8\")\n",
        "english_lst = file.read().split('\\n')\n",
        "english_dict = {}\n",
        "for line in english_lst:\n",
        "    key, value = line.split('\\t')\n",
        "    english_dict[key] = str(value)\n",
        "file.close()\n",
        "################\n",
        "#LOAD wrong words\n",
        "file = open('files/wrong-word.txt', 'r', encoding=\"utf8\")\n",
        "wrong_lst = file.read().split('\\n')\n",
        "file.close()\n",
        "#################\n",
        "#LOAD STOPWORDS\n",
        "file = open('files/vietnamese-stopwords.txt', 'r', encoding=\"utf8\")\n",
        "stopwords_lst = file.read().split('\\n')\n",
        "file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5P38CCWpyr6y"
      },
      "outputs": [],
      "source": [
        "def process_text(text, emoji_dict, teen_dict, wrong_lst):\n",
        "    document = text.lower()\n",
        "    document = document.replace(\"â€™\",'')\n",
        "    document = regex.sub(r'\\.+', \".\", document)\n",
        "    new_sentence =''\n",
        "    for sentence in sent_tokenize(document):\n",
        "        # if not(sentence.isascii()):\n",
        "        ###### CONVERT EMOJICON\n",
        "        sentence = ''.join(emoji_dict[word]+' ' if word in emoji_dict else word for word in list(sentence))\n",
        "        ###### CONVERT TEENCODE\n",
        "        sentence = ' '.join(teen_dict[word] if word in teen_dict else word for word in sentence.split())\n",
        "        ###### DEL Punctuation & Numbers\n",
        "        pattern = r'(?i)\\b[a-zÃ¡Ã áº£Ã£áº¡Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ã©Ã¨áº»áº½áº¹Ãªáº¿á»á»ƒá»…á»‡Ã³Ã²á»Ãµá»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£Ã­Ã¬á»‰Ä©á»‹ÃºÃ¹á»§Å©á»¥Æ°á»©á»«á»­á»¯á»±Ã½á»³á»·á»¹á»µÄ‘]+\\b'\n",
        "        sentence = ' '.join(regex.findall(pattern,sentence))\n",
        "        # ...\n",
        "        ###### DEL wrong words\n",
        "        sentence = ' '.join('' if word in wrong_lst else word for word in sentence.split())\n",
        "        new_sentence = new_sentence+ sentence + '. '\n",
        "    document = new_sentence\n",
        "    #print(document)\n",
        "    ###### DEL excess blank space\n",
        "    document = regex.sub(r'\\s+', ' ', document).strip()\n",
        "    #...\n",
        "    return document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wums9sysyr60"
      },
      "outputs": [],
      "source": [
        "# Chuáº©n hÃ³a unicode tiáº¿ng viá»‡t\n",
        "def loaddicchar():\n",
        "    uniChars = \"Ã Ã¡áº£Ã£áº¡Ã¢áº§áº¥áº©áº«áº­Äƒáº±áº¯áº³áºµáº·Ã¨Ã©áº»áº½áº¹Ãªá»áº¿á»ƒá»…á»‡Ä‘Ã¬Ã­á»‰Ä©á»‹Ã²Ã³á»Ãµá»Ã´á»“á»‘á»•á»—á»™Æ¡á»á»›á»Ÿá»¡á»£Ã¹Ãºá»§Å©á»¥Æ°á»«á»©á»­á»¯á»±á»³Ã½á»·á»¹á»µÃ€Ãáº¢Ãƒáº Ã‚áº¦áº¤áº¨áºªáº¬Ä‚áº°áº®áº²áº´áº¶ÃˆÃ‰áººáº¼áº¸ÃŠá»€áº¾á»‚á»„á»†ÄÃŒÃá»ˆÄ¨á»ŠÃ’Ã“á»Ã•á»ŒÃ”á»’á»á»”á»–á»˜Æ á»œá»šá»á» á»¢Ã™Ãšá»¦Å¨á»¤Æ¯á»ªá»¨á»¬á»®á»°á»²Ãá»¶á»¸á»´Ã‚Ä‚ÄÃ”Æ Æ¯\"\n",
        "    unsignChars = \"aaaaaaaaaaaaaaaaaeeeeeeeeeeediiiiiooooooooooooooooouuuuuuuuuuuyyyyyAAAAAAAAAAAAAAAAAEEEEEEEEEEEDIIIOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYAADOOU\"\n",
        "\n",
        "    dic = {}\n",
        "    char1252 = 'aÌ€|aÌ|aÌ‰|aÌƒ|aÌ£|Ã¢Ì€|Ã¢Ì|Ã¢Ì‰|Ã¢Ìƒ|Ã¢Ì£|ÄƒÌ€|ÄƒÌ|ÄƒÌ‰|ÄƒÌƒ|ÄƒÌ£|eÌ€|eÌ|eÌ‰|eÌƒ|eÌ£|ÃªÌ€|ÃªÌ|ÃªÌ‰|ÃªÌƒ|ÃªÌ£|iÌ€|iÌ|iÌ‰|iÌƒ|iÌ£|oÌ€|oÌ|oÌ‰|oÌƒ|oÌ£|Ã´Ì€|Ã´Ì|Ã´Ì‰|Ã´Ìƒ|Ã´Ì£|Æ¡Ì€|Æ¡Ì|Æ¡Ì‰|Æ¡Ìƒ|Æ¡Ì£|uÌ€|uÌ|uÌ‰|uÌƒ|uÌ£|Æ°Ì€|Æ°Ì|Æ°Ì‰|Æ°Ìƒ|Æ°Ì£|yÌ€|yÌ|yÌ‰|yÌƒ|yÌ£|AÌ€|AÌ|AÌ‰|AÌƒ|AÌ£|Ã‚Ì€|Ã‚Ì|Ã‚Ì‰|Ã‚Ìƒ|Ã‚Ì£|Ä‚Ì€|Ä‚Ì|Ä‚Ì‰|Ä‚Ìƒ|Ä‚Ì£|EÌ€|EÌ|EÌ‰|EÌƒ|EÌ£|ÃŠÌ€|ÃŠÌ|ÃŠÌ‰|ÃŠÌƒ|ÃŠÌ£|IÌ€|IÌ|IÌ‰|IÌƒ|IÌ£|OÌ€|OÌ|OÌ‰|OÌƒ|OÌ£|Ã”Ì€|Ã”Ì|Ã”Ì‰|Ã”Ìƒ|Ã”Ì£|Æ Ì€|Æ Ì|Æ Ì‰|Æ Ìƒ|Æ Ì£|UÌ€|UÌ|UÌ‰|UÌƒ|UÌ£|Æ¯Ì€|Æ¯Ì|Æ¯Ì‰|Æ¯Ìƒ|Æ¯Ì£|YÌ€|YÌ|YÌ‰|YÌƒ|YÌ£'.split(\n",
        "        '|')\n",
        "    charutf8 = \"Ã |Ã¡|áº£|Ã£|áº¡|áº§|áº¥|áº©|áº«|áº­|áº±|áº¯|áº³|áºµ|áº·|Ã¨|Ã©|áº»|áº½|áº¹|á»|áº¿|á»ƒ|á»…|á»‡|Ã¬|Ã­|á»‰|Ä©|á»‹|Ã²|Ã³|á»|Ãµ|á»|á»“|á»‘|á»•|á»—|á»™|á»|á»›|á»Ÿ|á»¡|á»£|Ã¹|Ãº|á»§|Å©|á»¥|á»«|á»©|á»­|á»¯|á»±|á»³|Ã½|á»·|á»¹|á»µ|Ã€|Ã|áº¢|Ãƒ|áº |áº¦|áº¤|áº¨|áºª|áº¬|áº°|áº®|áº²|áº´|áº¶|Ãˆ|Ã‰|áºº|áº¼|áº¸|á»€|áº¾|á»‚|á»„|á»†|ÃŒ|Ã|á»ˆ|Ä¨|á»Š|Ã’|Ã“|á»|Ã•|á»Œ|á»’|á»|á»”|á»–|á»˜|á»œ|á»š|á»|á» |á»¢|Ã™|Ãš|á»¦|Å¨|á»¤|á»ª|á»¨|á»¬|á»®|á»°|á»²|Ã|á»¶|á»¸|á»´\".split(\n",
        "        '|')\n",
        "    for i in range(len(char1252)):\n",
        "        dic[char1252[i]] = charutf8[i]\n",
        "    return dic\n",
        "\n",
        "# ÄÆ°a toÃ n bá»™ dá»¯ liá»‡u qua hÃ m nÃ y Ä‘á»ƒ chuáº©n hÃ³a láº¡i\n",
        "def covert_unicode(txt):\n",
        "    dicchar = loaddicchar()\n",
        "    return regex.sub(\n",
        "        r'aÌ€|aÌ|aÌ‰|aÌƒ|aÌ£|Ã¢Ì€|Ã¢Ì|Ã¢Ì‰|Ã¢Ìƒ|Ã¢Ì£|ÄƒÌ€|ÄƒÌ|ÄƒÌ‰|ÄƒÌƒ|ÄƒÌ£|eÌ€|eÌ|eÌ‰|eÌƒ|eÌ£|ÃªÌ€|ÃªÌ|ÃªÌ‰|ÃªÌƒ|ÃªÌ£|iÌ€|iÌ|iÌ‰|iÌƒ|iÌ£|oÌ€|oÌ|oÌ‰|oÌƒ|oÌ£|Ã´Ì€|Ã´Ì|Ã´Ì‰|Ã´Ìƒ|Ã´Ì£|Æ¡Ì€|Æ¡Ì|Æ¡Ì‰|Æ¡Ìƒ|Æ¡Ì£|uÌ€|uÌ|uÌ‰|uÌƒ|uÌ£|Æ°Ì€|Æ°Ì|Æ°Ì‰|Æ°Ìƒ|Æ°Ì£|yÌ€|yÌ|yÌ‰|yÌƒ|yÌ£|AÌ€|AÌ|AÌ‰|AÌƒ|AÌ£|Ã‚Ì€|Ã‚Ì|Ã‚Ì‰|Ã‚Ìƒ|Ã‚Ì£|Ä‚Ì€|Ä‚Ì|Ä‚Ì‰|Ä‚Ìƒ|Ä‚Ì£|EÌ€|EÌ|EÌ‰|EÌƒ|EÌ£|ÃŠÌ€|ÃŠÌ|ÃŠÌ‰|ÃŠÌƒ|ÃŠÌ£|IÌ€|IÌ|IÌ‰|IÌƒ|IÌ£|OÌ€|OÌ|OÌ‰|OÌƒ|OÌ£|Ã”Ì€|Ã”Ì|Ã”Ì‰|Ã”Ìƒ|Ã”Ì£|Æ Ì€|Æ Ì|Æ Ì‰|Æ Ìƒ|Æ Ì£|UÌ€|UÌ|UÌ‰|UÌƒ|UÌ£|Æ¯Ì€|Æ¯Ì|Æ¯Ì‰|Æ¯Ìƒ|Æ¯Ì£|YÌ€|YÌ|YÌ‰|YÌƒ|YÌ£',\n",
        "        lambda x: dicchar[x.group()], txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auHk1SStyr61"
      },
      "outputs": [],
      "source": [
        "def process_special_word(text):\n",
        "    # cÃ³ thá»ƒ cÃ³ nhiá»u tá»« Ä‘áº·c biá»‡t cáº§n rÃ¡p láº¡i vá»›i nhau\n",
        "    new_text = ''\n",
        "    text_lst = text.split()\n",
        "    i= 0\n",
        "    # khÃ´ng, cháº³ng, cháº£...\n",
        "    if 'khÃ´ng' in text_lst:\n",
        "        while i <= len(text_lst) - 1:\n",
        "            word = text_lst[i]\n",
        "            #print(word)\n",
        "            #print(i)\n",
        "            if  word == 'khÃ´ng':\n",
        "                next_idx = i+1\n",
        "                if next_idx <= len(text_lst) -1:\n",
        "                    word = word +'_'+ text_lst[next_idx]\n",
        "                i= next_idx + 1\n",
        "            else:\n",
        "                i = i+1\n",
        "            new_text = new_text + word + ' '\n",
        "    else:\n",
        "        new_text = text\n",
        "    return new_text.strip()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "# HÃ m Ä‘á»ƒ chuáº©n hÃ³a cÃ¡c tá»« cÃ³ kÃ½ tá»± láº·p\n",
        "def normalize_repeated_characters(text):\n",
        "    # Thay tháº¿ má»i kÃ½ tá»± láº·p liÃªn tiáº¿p báº±ng má»™t kÃ½ tá»± Ä‘Ã³\n",
        "    # VÃ­ dá»¥: \"ngonnnn\" thÃ nh \"ngon\", \"thiá»‡tttt\" thÃ nh \"thiá»‡t\"\n",
        "    return re.sub(r'(.)\\1+', r'\\1', text)\n",
        "\n",
        "# Ãp dá»¥ng hÃ m chuáº©n hÃ³a cho vÄƒn báº£n\n",
        "# print(normalize_repeated_characters(example))"
      ],
      "metadata": {
        "id": "gq6q_AkInTO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFuQF1gMyr62"
      },
      "outputs": [],
      "source": [
        "def process_postag_thesea(text):\n",
        "    new_document = ''\n",
        "    for sentence in sent_tokenize(text):\n",
        "        sentence = sentence.replace('.','')\n",
        "        ###### POS tag\n",
        "        lst_word_type = ['N','Np','A','AB','V','VB','VY','R']\n",
        "        # lst_word_type = ['A','AB','V','VB','VY','R']\n",
        "        sentence = ' '.join( word[0] if word[1].upper() in lst_word_type else '' for word in pos_tag(process_special_word(word_tokenize(sentence, format=\"text\"))))\n",
        "        new_document = new_document + sentence + ' '\n",
        "    ###### DEL excess blank space\n",
        "    new_document = regex.sub(r'\\s+', ' ', new_document).strip()\n",
        "    return new_document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSX3s05Zyr62"
      },
      "outputs": [],
      "source": [
        "def remove_stopword(text, stopwords):\n",
        "    ###### REMOVE stop words\n",
        "    document = ' '.join('' if word in stopwords else word for word in text.split())\n",
        "    #print(document)\n",
        "    ###### DEL excess blank space\n",
        "    document = regex.sub(r'\\s+', ' ', document).strip()\n",
        "    return document"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Táº¡o thÃªm cÃ¡c cá»™t má»›i dá»±a trÃªn viá»‡c Ä‘áº¿m tá»«/icon positive vÃ  negative\n",
        "- Táº¡o danh sÃ¡ch cÃ¡c tá»« positive -> lÆ°u vÃ o file positive_VN.txt (má»—i tá»« trÃªn 1 dÃ²ng)\n",
        "- Táº¡o danh sÃ¡ch cÃ¡c tá»« negative -> lÆ°u vÃ o file negative_VN.txt (má»—i tá»« trÃªn 1 dÃ²ng)\n",
        "- Táº¡o danh sÃ¡ch cÃ¡c positve emojis -> lÆ°u vÃ o file positive_emoji.txt (má»—i icon trÃªn 1 dÃ²ng)\n",
        "- Táº¡o danh sÃ¡ch cÃ¡c negative emojis -> lÆ°u vÃ o file negative_emoji.txt (má»—i icon trÃªn 1 dÃ²ng)\n",
        "- Äá»c vÃ o cÃ¡c list tÆ°Æ¡ng á»©ng\n",
        "- Viáº¿t function Ä‘á»ƒ Ä‘á»c 1 chuá»—i -> Ä‘áº¿m sá»‘ lÆ°á»£ng positive words/ emojis hoáº·c  negative words/ ememojis, danh sÃ¡ch tá»« káº¿t quáº£\n",
        "- Táº¡o ra 2 cá»™t má»›i cho dataframe: negative_count/ positive_count\n",
        "- ..."
      ],
      "metadata": {
        "id": "ojRtRJSx8CZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# bá»• sung thÃªm\n",
        "positive_words = [\n",
        "    \"thÃ­ch\", \"tá»‘t\", \"xuáº¥t sáº¯c\", \"tuyá»‡t vá»i\", \"tuyá»‡t háº£o\", \"Ä‘áº¹p\", \"á»•n\", \"ngon\",\n",
        "    \"hÃ i lÃ²ng\", \"Æ°ng Ã½\", \"hoÃ n háº£o\", \"cháº¥t lÆ°á»£ng\", \"thÃº vá»‹\", \"nhanh\",\n",
        "    \"tiá»‡n lá»£i\", \"dá»… sá»­ dá»¥ng\", \"hiá»‡u quáº£\", \"áº¥n tÆ°á»£ng\",\n",
        "    \"ná»•i báº­t\", \"táº­n hÆ°á»Ÿng\", \"tá»‘n Ã­t thá»i gian\", \"thÃ¢n thiá»‡n\", \"háº¥p dáº«n\",\n",
        "    \"gá»£i cáº£m\", \"tÆ°Æ¡i má»›i\", \"láº¡ máº¯t\", \"cao cáº¥p\", \"Ä‘á»™c Ä‘Ã¡o\",\n",
        "    \"há»£p kháº©u vá»‹\", \"ráº¥t tá»‘t\", \"ráº¥t thÃ­ch\", \"táº­n tÃ¢m\", \"Ä‘Ã¡ng tin cáº­y\", \"Ä‘áº³ng cáº¥p\",\n",
        "    \"háº¥p dáº«n\", \"an tÃ¢m\", \"khÃ´ng thá»ƒ cÆ°á»¡ng láº¡i\", \"thá»a mÃ£n\", \"thÃºc Ä‘áº©y\",\n",
        "    \"cáº£m Ä‘á»™ng\", \"phá»¥c vá»¥ tá»‘t\", \"lÃ m hÃ i lÃ²ng\", \"gÃ¢y áº¥n tÆ°á»£ng\", \"ná»•i trá»™i\",\n",
        "    \"sÃ¡ng táº¡o\", \"quÃ½ bÃ¡u\", \"phÃ¹ há»£p\", \"táº­n tÃ¢m\",\n",
        "    \"hiáº¿m cÃ³\", \"cáº£i thiá»‡n\", \"hoÃ  nhÃ£\", \"chÄƒm chá»‰\", \"cáº©n tháº­n\",\n",
        "    \"vui váº»\", \"sÃ¡ng sá»§a\", \"hÃ o há»©ng\", \"Ä‘am mÃª\", \"vá»«a váº·n\", \"Ä‘Ã¡ng tiá»n\"\n",
        "]"
      ],
      "metadata": {
        "id": "fl9Jse0l9buR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(positive_words)"
      ],
      "metadata": {
        "id": "U1qEeyJv9tf4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2c8acb8-6f5e-4423-8dfe-587a8659790c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "59"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# bá»• sung thÃªm...\n",
        "negative_words = [\n",
        "    \"kÃ©m\", \"tá»‡\", \"Ä‘au\", \"xáº¥u\", \"dá»Ÿ\", \"á»©c\",\n",
        "    \"buá»“n\", \"rá»‘i\", \"thÃ´\", \"lÃ¢u\", \"chÃ¡n\"\n",
        "    \"tá»‘i\", \"chÃ¡n\", \"Ã­t\", \"má»\", \"má»ng\",\n",
        "    \"lá»ng láº»o\", \"khÃ³\", \"cÃ¹i\", \"yáº¿u\",\n",
        "    \"kÃ©m cháº¥t lÆ°á»£ng\", \"khÃ´ng thÃ­ch\", \"khÃ´ng thÃº vá»‹\", \"khÃ´ng á»•n\",\n",
        "    \"khÃ´ng há»£p\", \"khÃ´ng Ä‘Ã¡ng tin cáº­y\", \"khÃ´ng chuyÃªn nghiá»‡p\",\n",
        "    \"khÃ´ng pháº£n há»“i\", \"khÃ´ng an toÃ n\", \"khÃ´ng phÃ¹ há»£p\", \"khÃ´ng thÃ¢n thiá»‡n\", \"khÃ´ng linh hoáº¡t\", \"khÃ´ng Ä‘Ã¡ng giÃ¡\",\n",
        "    \"khÃ´ng áº¥n tÆ°á»£ng\", \"khÃ´ng tá»‘t\", \"cháº­m\", \"khÃ³ khÄƒn\", \"phá»©c táº¡p\",\n",
        "    \"khÃ³ hiá»ƒu\", \"khÃ³ chá»‹u\", \"gÃ¢y khÃ³ dá»…\", \"rÆ°á»m rÃ \", \"khÃ³ truy cáº­p\",\n",
        "    \"tháº¥t báº¡i\", \"tá»“i tá»‡\", \"khÃ³ xá»­\", \"khÃ´ng thá»ƒ cháº¥p nháº­n\", \"tá»“i tá»‡\",\"khÃ´ng rÃµ rÃ ng\",\n",
        "    \"khÃ´ng cháº¯c cháº¯n\", \"rá»‘i ráº¯m\", \"khÃ´ng tiá»‡n lá»£i\", \"khÃ´ng Ä‘Ã¡ng tiá»n\", \"chÆ°a Ä‘áº¹p\", \"khÃ´ng Ä‘áº¹p\"\n",
        "]"
      ],
      "metadata": {
        "id": "lQLgtqZv9Ye5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(negative_words)"
      ],
      "metadata": {
        "id": "Heg5npFz-ZvZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9876988-a428-41f5-b088-4d14c3c6d152"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "54"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_words(document, list_of_words):\n",
        "    document_lower = document.lower()\n",
        "    word_count = 0\n",
        "    word_list = []\n",
        "\n",
        "    for word in list_of_words:\n",
        "        if word in document_lower:\n",
        "            print(word)\n",
        "            word_count += document_lower.count(word)\n",
        "            word_list.append(word)\n",
        "\n",
        "    return word_count, word_list"
      ],
      "metadata": {
        "id": "Z4utzELqC-nH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count emojis positive and negative\n",
        "# bá»• sung thÃªm...\n",
        "negative_emojis = [\n",
        "    \"ğŸ˜\", \"ğŸ˜”\", \"ğŸ™\", \"â˜¹ï¸\", \"ğŸ˜•\",\n",
        "    \"ğŸ˜¢\", \"ğŸ˜­\", \"ğŸ˜–\", \"ğŸ˜£\", \"ğŸ˜©\",\n",
        "    \"ğŸ˜ \", \"ğŸ˜¡\", \"ğŸ¤¬\", \"ğŸ˜¤\", \"ğŸ˜°\",\n",
        "    \"ğŸ˜¨\", \"ğŸ˜±\", \"ğŸ˜ª\", \"ğŸ˜“\", \"ğŸ¥º\",\n",
        "    \"ğŸ˜’\", \"ğŸ™„\", \"ğŸ˜‘\", \"ğŸ˜¬\", \"ğŸ˜¶\",\n",
        "    \"ğŸ¤¯\", \"ğŸ˜³\", \"ğŸ¤¢\", \"ğŸ¤®\", \"ğŸ¤•\",\n",
        "    \"ğŸ¥´\", \"ğŸ¤”\", \"ğŸ˜·\", \"ğŸ™…â€â™‚ï¸\", \"ğŸ™…â€â™€ï¸\",\n",
        "    \"ğŸ™†â€â™‚ï¸\", \"ğŸ™†â€â™€ï¸\", \"ğŸ™‡â€â™‚ï¸\", \"ğŸ™‡â€â™€ï¸\", \"ğŸ¤¦â€â™‚ï¸\",\n",
        "    \"ğŸ¤¦â€â™€ï¸\", \"ğŸ¤·â€â™‚ï¸\", \"ğŸ¤·â€â™€ï¸\", \"ğŸ¤¢\", \"ğŸ¤§\",\n",
        "    \"ğŸ¤¨\", \"ğŸ¤«\", \"ğŸ‘\", \"ğŸ‘Š\", \"âœŠ\", \"ğŸ¤›\", \"ğŸ¤œ\",\n",
        "    \"ğŸ¤š\", \"ğŸ–•\"\n",
        "]"
      ],
      "metadata": {
        "id": "nBYIj19XFJkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(negative_emojis)"
      ],
      "metadata": {
        "id": "KmFhyhI9FkfG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a112cd2b-d9cb-497e-e29d-00b4237b016a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "54"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "positive_emojis = [\n",
        "    \"ğŸ˜„\", \"ğŸ˜ƒ\", \"ğŸ˜€\", \"ğŸ˜\", \"ğŸ˜†\",\n",
        "    \"ğŸ˜…\", \"ğŸ¤£\", \"ğŸ˜‚\", \"ğŸ™‚\", \"ğŸ™ƒ\",\n",
        "    \"ğŸ˜‰\", \"ğŸ˜Š\", \"ğŸ˜‡\", \"ğŸ¥°\", \"ğŸ˜\",\n",
        "    \"ğŸ¤©\", \"ğŸ˜˜\", \"ğŸ˜—\", \"ğŸ˜š\", \"ğŸ˜™\",\n",
        "    \"ğŸ˜‹\", \"ğŸ˜›\", \"ğŸ˜œ\", \"ğŸ¤ª\", \"ğŸ˜\",\n",
        "    \"ğŸ¤—\", \"ğŸ¤­\", \"ğŸ¥³\", \"ğŸ˜Œ\", \"ğŸ˜\",\n",
        "    \"ğŸ¤“\", \"ğŸ§\", \"ğŸ‘\", \"ğŸ¤\", \"ğŸ™Œ\", \"ğŸ‘\", \"ğŸ‘‹\",\n",
        "    \"ğŸ¤™\", \"âœ‹\", \"ğŸ–ï¸\", \"ğŸ‘Œ\", \"ğŸ¤\",\n",
        "    \"âœŒï¸\", \"ğŸ¤Ÿ\", \"ğŸ‘ˆ\", \"ğŸ‘‰\", \"ğŸ‘†\",\n",
        "    \"ğŸ‘‡\", \"â˜ï¸\"\n",
        "]"
      ],
      "metadata": {
        "id": "CrCv5JJVGSi3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}